# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wB5QJEboTFC1GD5YTK5Z6El7Ok_e6Zn0

# **1.Explaratory Data Analysis**
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import pickle


#Uploading dataset
file_path = '/home/osama/Documents/frist_task/first inten project.csv'

#read dataset
data = pd.read_csv(file_path)

data.head()

data.shape

# Show features (columns) of the DataFrame
features = data.columns
print("Features of the CSV file:")
for feature in features:
    print(feature)

data.dtypes

data.describe()

# Filter categorical features
categorical_features = data.select_dtypes(include=['object']).columns
categorical_features

#checking for missing values
data.isna().sum()

"""# **2.Plotting**"""

import pandas as pd
import matplotlib.pyplot as plt


# Exclude specific columns from the loop
exclude_columns = ['Booking_ID', 'date of reservation']

# Plot each variable
for column in data.columns:
    if column not in exclude_columns:  # Check if the column should be excluded
        plt.figure(figsize=(8, 6))  # Set the size of the figure
        if data[column].dtype == 'object':
            # For categorical variables, plot a bar plot
            data[column].value_counts().plot(kind='bar')
            plt.title(f'Count of {column}')
            plt.xlabel(column)
            plt.ylabel('Count')
        else:
            # For numerical variables, plot a histogram
            data[column].plot(kind='hist', bins=20)
            plt.title(f'Distribution of {column}')
            plt.xlabel(column)
            plt.ylabel('Frequency')
        plt.show()

#box plot for checking the existence of outliers

# Filter columns to include in the box plot
columns_to_plot = [column for column in data.columns if data[column].dtype != 'object']

# Plot box plots for each variable
for column in columns_to_plot:
    plt.figure(figsize=(8, 6))  # Set the size of the figure
    sns.boxplot(x=data[column])
    plt.title(f'Box Plot of {column}')
    plt.xlabel(column)
    plt.show()

#pie charts for categorical features

# Exclude specific columns from the pie chart
exclude_columns = ['Booking_ID', 'date of reservation']

# Filter categorical columns to include in the pie chart
categorical_columns = [column for column in data.columns if column not in exclude_columns and data[column].dtype == 'object']

# Plot pie charts for each categorical variable
for column in categorical_columns:
    plt.figure(figsize=(8, 6))  # Set the size of the figure
    category_counts = data[column].value_counts()
    plt.pie(category_counts, labels=category_counts.index, autopct='%1.1f%%', startangle=140)
    plt.title(f'Pie Chart of {column}')
    plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
    plt.show()

#Correlation matrix for numerical variables

# Filter numerical columns to include in the correlation matrix
numerical_columns = [column for column in data.columns if column not in exclude_columns and pd.api.types.is_numeric_dtype(data[column])]

# Compute the correlation matrix
correlation_matrix = data[numerical_columns].corr()

# Plot the correlation matrix as a heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=0.5)
plt.title('Correlation Matrix of Numerical Variables')
plt.show()

"""Cleansing

"""

#Checking for duplicate values

# Check for duplicate rows
duplicate_rows = data.duplicated()

# Count the number of duplicate rows
num_duplicates = duplicate_rows.sum()

# Print the number of duplicate rows
print("Number of duplicate rows:", num_duplicates)

# Check for missing values in each column
missing_values_in_columns = data.isna().any()

# Check if there are any missing values in the entire DataFrame
any_missing_values = missing_values_in_columns.any()

# Print the result
if any_missing_values:
    print("There are missing values in the DataFrame.")
else:
    print("There are no missing values in the DataFrame.")

"""# **3.Preprocessing**"""

data.isnull().sum()

#Remove outliers
# Define a function to detect outliers using IQR method

# Select only numerical columns
numerical_data = data.select_dtypes(include=['int64', 'float64'])

def detect_outliers_columns(data, threshold=1.5):
    outliers_columns = []
    for column in data.columns:
        if pd.api.types.is_numeric_dtype(data[column]):
            q1 = data[column].quantile(0.25)
            q3 = data[column].quantile(0.75)
            iqr = q3 - q1
            lower_bound = q1 - threshold * iqr
            upper_bound = q3 + threshold * iqr
            column_outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]
            if not column_outliers.empty:
                outliers_columns.append(column)
    return outliers_columns

# Detect columns with outliers in the DataFrame
outliers_columns = detect_outliers_columns(numerical_data)

# Print the names of columns containing outliers
print("Columns containing outliers:", outliers_columns)

# Define a function to replace outliers with the median value
def replace_outliers(data, threshold=1.5):
    replaced_data = data.copy()
    for column in data.columns:
        if pd.api.types.is_numeric_dtype(data[column]):
            q1 = data[column].quantile(0.25)
            q3 = data[column].quantile(0.75)
            iqr = q3 - q1
            lower_bound = q1 - threshold * iqr
            upper_bound = q3 + threshold * iqr
            # Replace outliers with median value
            replaced_data[column] = data[column].apply(lambda x: data[column].median() if x < lower_bound or x > upper_bound else x)
    return replaced_data

# Replace outliers in the DataFrame
data_without_outliers = replace_outliers(numerical_data)

# Print the DataFrame after replacing outliers
print("DataFrame after replacing outliers:")
print(data_without_outliers)

# ENcode categorical features
# Assuming 'categorical_features' is a DataFrame containing your categorical features

# Create lists to store binary and multi-class features
binary_features = []
multi_class_features = []


# Get the columns with categorical data types
categorical_columns = data.select_dtypes(include=['object', 'category']).columns.tolist()

print("Categorical features:", categorical_features)

# Iterate through each column (feature) in the DataFrame
for column in categorical_columns:
    # Check the number of unique values in the column
    num_unique_values = len(data[column].unique())

    # If the number of unique values is 2, consider it as a binary feature
    if num_unique_values == 2:
        binary_features.append(column)
    # Otherwise, consider it as a multi-class feature
    else:
        multi_class_features.append(column)

# Print the lists of binary and multi-class features
print("Binary features:", binary_features)
print("Multi-class features:", multi_class_features)

# Initialize LabelEncoder

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()

# Iterate through each column in the list of multi-class columns
for column in multi_class_features:
    # Fit and transform the column with label encoding
    data[column] = label_encoder.fit_transform(data[column])

print(data)

import pandas as pd

# Assuming 'data' is your DataFrame and 'booking_status' is the name of the target variable

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Perform label encoding on the 'booking status' column
data['booking status'] = label_encoder.fit_transform(data['booking status'])

print(data)

data.drop(columns=['Booking_ID'], inplace=True)

"""# **4.Use feature selection techniques:**

"""

X=data.drop(columns=['booking status'])
y=data['booking status']

"""1.Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel

clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X, y)
sfm = SelectFromModel(clf, threshold=0.05)  # Adjust threshold as needed
sfm.fit(X, y)
selected_features1 = X.columns[sfm.get_support()]
print("Selected features:", selected_features1)

"""# **5.Splitting the data for train and test**"""

from sklearn.model_selection import train_test_split

# Split the data into features (X) and target variable (y)
X=data[selected_features1]
y=data['booking status']


# Split the data into train and test sets using stratified splitting
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Print the shapes of the train and test sets
print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

"""# **6.Bulied a Model(using Random Forest Classifier):**

6.RandomForestClassifier
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import accuracy_score, f1_score
# Assuming X contains your feature matrix and y contains your target variable
# Replace X and y with your actual feature matrix and target variable


# Initialize the Random Forest classifier
clf_rf = RandomForestClassifier(n_estimators=100, random_state=42)  # Example parameters, you can adjust as needed

# Fit the classifier to the training data
clf_rf.fit(X_train, y_train)

# Predict the target variable for the test data
y_pred_rf = clf_rf.predict(X_test)

# Calculate accuracy
accuracy_rf = accuracy_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)
print("Accuracy of Random Forest classifier:", accuracy_rf)
print("F1 Score of Random Forest classifier:", f1_rf)

"""7.AdaBoostClassifier"""

import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score

# Assuming X contains the features and y contains the target (Booking status)
# Split the data into training and testing sets

# Define LightGBM parameters
params = {
    'objective': 'binary',
    'metric': 'binary_error',  # Use binary error as the evaluation metric
    'num_leaves': 31,  # Maximum tree leaves for base learners
    'learning_rate': 0.05,  # Learning rate
    'feature_fraction': 0.9,  # Feature fraction
    'bagging_fraction': 0.8,  # Bagging fraction
    'bagging_freq': 5,  # Bagging frequency
    'verbose': 0  # Verbosity
}

# Convert the dataset into LightGBM format
train_data = lgb.Dataset(X_train, label=y_train)
test_data = lgb.Dataset(X_test, label=y_test)

# Train the LightGBM model
num_round = 100  # Number of boosting rounds
bst = lgb.train(params, train_data, num_round, valid_sets=[test_data])

# Make predictions on the test set
y_pred = bst.predict(X_test, num_iteration=num_round)

# Convert probabilities to binary predictions
y_pred_binary = [1 if pred >= 0.5 else 0 for pred in y_pred]

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred_binary)

# Calculate F1 score
f1 = f1_score(y_test, y_pred_binary)

print("Accuracy:", accuracy)
print("F1 Score:", f1)

import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import accuracy_score, f1_score

# Assuming X contains the features and y contains the target (Booking status)
# Split the data into training and testing sets

# Define XGBoost parameters
params = {
    'objective': 'binary:logistic',  # Binary classification
    'eval_metric': 'error',  # Evaluation metric
    'eta': 0.1,  # Learning rate
    'max_depth': 6,  # Maximum depth of a tree
    'min_child_weight': 1,  # Minimum sum of instance weight needed in a child
    'subsample': 0.8,  # Subsample ratio of the training instances
    'colsample_bytree': 0.8,  # Subsample ratio of columns when constructing each tree
    'gamma': 0,  # Minimum loss reduction required to make a further partition
    'lambda': 1,  # L2 regularization term on weights
    'alpha': 0,  # L1 regularization term on weights
    'seed': 42  # Random seed
}

# Convert the dataset into XGBoost DMatrix format
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Train the XGBoost model
num_rounds = 100  # Number of boosting rounds
model = xgb.train(params, dtrain, num_rounds)

# Make predictions on the test set
y_pred_prob = model.predict(dtest)
y_pred = [1 if pred >= 0.5 else 0 for pred in y_pred_prob]

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred_binary)
print("XGBoost Accuracy:", accuracy)
print("F1 Score:", f1)

import numpy as np
from sklearn.metrics import accuracy_score

# Assuming y_pred_prob contains the predicted probabilities and y_true contains the true labels

# Define a list of thresholds to try
thresholds = np.arange(0.1, 1.0, 0.1)  # Adjust the range as needed

best_accuracy = 0
best_threshold = 0

# Iterate over each threshold and calculate accuracy
for threshold in thresholds:
    # Convert probabilities to binary predictions based on the threshold
    y_pred = [1 if pred >= threshold else 0 for pred in y_pred_prob]

    # Calculate accuracy
    accuracy =  accuracy_score(y_test, y_pred)

    # Check if this threshold gives a higher accuracy
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_threshold = threshold

print("Best Threshold:", best_threshold)
print("Best Accuracy:", best_accuracy)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Assuming X_train, X_test, y_train, y_test are already defined

# Train a Random Forest classifier with optimized parameters
 # Example parameters, you can adjust as needed

# Predict probabilities on the test set
y_pred_proba = clf_rf.predict_proba(X_test)

# Determine predictions based on the optimized threshold
optimal_threshold = 0.5# Example threshold, replace with your optimized value
y_pred_opt = (y_pred_proba[:, 1] >= optimal_threshold).astype(int)

# Calculate accuracy
accuracy_rf = accuracy_score(y_test, y_pred_opt)
print("Accuracy of Random Forest classifier with optimized threshold:", accuracy_rf)

import xgboost as xgb
from sklearn.metrics import accuracy_score

# Assuming X_train, X_test, y_train, y_test are already defined

# Define XGBoost parameters
params = {
    'objective': 'binary:logistic',  # Binary classification
    'eval_metric': 'error',  # Evaluation metric
    'eta': 0.1,  # Learning rate
    'max_depth': 6,  # Maximum depth of a tree
    'min_child_weight': 1,  # Minimum sum of instance weight needed in a child
    'subsample': 0.8,  # Subsample ratio of the training instances
    'colsample_bytree': 0.8,  # Subsample ratio of columns when constructing each tree
    'gamma': 0,  # Minimum loss reduction required to make a further partition
    'lambda': 1,  # L2 regularization term on weights
    'alpha': 0,  # L1 regularization term on weights
    'seed': 42  # Random seed
}

# Convert the dataset into XGBoost DMatrix format
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Train the XGBoost model
num_rounds = 100  # Number of boosting rounds
model = xgb.train(params, dtrain, num_rounds)

# Make predictions on the test set
y_pred_prob = model.predict(dtest)

# Apply the best threshold
best_threshold = 0.5 # Use the best threshold obtained
y_pred = (y_pred_prob >= best_threshold).astype(int)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("XGBoost Accuracy with Best Threshold:", accuracy)

from flask import Flask, request, jsonify
import pandas as pd
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectFromModel
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score

app = Flask(__name__)

# Load the trained model
import joblib

# Train your model (example)
model = RandomForestClassifier()
model.fit(X_train, y_train)

# Save the trained model to a file
joblib.dump(model, 'model.pkl')
# Define a function to replace outliers with the median value
def replace_outliers(data, threshold=1.5):
    replaced_data = data.copy()
    for column in data.columns:
        if pd.api.types.is_numeric_dtype(data[column]):
            q1 = data[column].quantile(0.25)
            q3 = data[column].quantile(0.75)
            iqr = q3 - q1
            lower_bound = q1 - threshold * iqr
            upper_bound = q3 + threshold * iqr
            # Replace outliers with median value
            replaced_data[column] = data[column].apply(lambda x: data[column].median() if x < lower_bound or x > upper_bound else x)
    return replaced_data

# Prediction endpoint
@app.route('/predict', methods=['POST'])
def predict():
    # Get input data from the request
    data = request.get_json()

    # Convert JSON data to DataFrame
    df = pd.DataFrame(data)

    # Replace outliers
    df = replace_outliers(df)

    # Feature selection
    selected_features = ['number of week nights', 'lead time','market segment type','average price ','special requests', 'date of reservation']  # Replace with your selected features

    # Select only the selected features
    X = df[selected_features]

    # Make predictions using the loaded model
    predictions = model.predict(X)

    # Return the predictions as JSON response
    return jsonify(predictions.tolist())

if __name__ == '__main__':
    app.run(debug=True)